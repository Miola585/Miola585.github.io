---
layout: post
title: "Day 24 â€“ Optimization and Predicition"
date: 2025-06-27
author: Michelle Oladele-Kuyoro
permalink: /day24.html
tags: [ "AdamW", "MobileNetV2", "jupyter notebook"]

what_i_learned: |
  I started my day by checking up on the models that I had left to run overnight. The optimizers I had been using, Adam and SGD, had not been giving me satisfactory results because they kept overfitting. I was able to try running Lion today but could not complete it since i ran it on the colab GPU. I tried running it locally, but Yusrah let me know that Lion does not run with the version of tensorflow i am using to run locally so I had to try a different optimizer.

  Since the Lion optimizer didn't quite workout as I had hoped, I decided to go with AdamW. This optimizer is a variant of Adam taht implements weight decay, leading to better generalization which the model has been struggling with over these past few weeks. It would often memorize the pictures instead of just making inferences from the pictures. AdamW produced better results in addition to class weights, early stopping, and model checkpoint.

blockers: |
   Installing AdamW to the colab notebook was very different as it is not in the keras library, so it was a bit of a struggle getting it to work.

reflection: |
  It was very productive in the lab today, I was able to run more models, I am currently running one with 60 epochs. I am very satisfied with the work that I was able to do today. The models that I ran today containing 60 Epochs each and they both gave very high accuracies, precision, recall, and f1 scores. It's a relief that I was able to make more progress after these few weeks. I was also able to work with my group members on the mid summer presentation. I look forward to what I will be able to achieve tommorow.
---
