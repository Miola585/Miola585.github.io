---
layout: post
title: "Day 23 â€“ Optimization and Predicition Contined"
date: 2025-06-26
author: Michelle Oladele-Kuyoro
permalink: /day23.html
tags: [ "AdamW", "MobileNetV2", "jupyter notebook", "Anaconda"]

what_i_learned: |
  I started my day by checking up on the models that I had left to run overnight. The optimizers I had been using, Adam and SGD, had not been giving me satisfactory results because they kept overfitting. I was able to try running Lion today but could not complete it since i ran it on the colab GPU. I was very surprised to find out that line optimize that still had not been completed when I got back to check on them today. I tried running it locally, but Yusrah let me know that Lion does not run with the version of tensorflow i am using to run locally so I had to try a different optimizer.

  Since the Lion optimizer didn't quite workout as I had hoped, I decided to go with AdamW. This optimizer is a variant of Adam taht implements weight decay, leading to better generalization which the model has been struggling with over these past few weeks. It would often memorize the pictures instead of just making inferences from the pictures. AdamW produced better results in addition to class weights, early stopping, and model checkpoint.

blockers: |
   Installing AdamW to the colab notebook was very different as it is not in the keras library, so it was a bit of a struggle getting it to work.

reflection: |
   I am very satisfied with the work that I was able to do today. The models that I ran today containing 50 Epochs each and they both gave very high accuracies, precision, recall, and f1 scores. It's a relief that I was able to make more progress after these few weeks. I continued working on my mid summer presentation. I look forward to what I will be able to achieve tommorow.
---
